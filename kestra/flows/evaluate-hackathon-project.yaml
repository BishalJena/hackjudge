# Kestra Workflow: HackJudge AI Evaluation Pipeline
# Using WorkingDirectory pattern for shared filesystem between tasks

id: evaluate-hackathon-project
namespace: hackjudge

description: |
  Main evaluation pipeline for HackJudge AI.
  Clones a GitHub repository, analyzes code, and generates a readiness report.

labels:
  project: hackjudge
  type: evaluation

tasks:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Main Evaluation Pipeline - All tasks share the same directory
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  - id: evaluation_pipeline
    type: io.kestra.plugin.core.flow.WorkingDirectory
    outputFiles:
      - artifacts/**
    tasks:
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # STEP 1: Setup directories
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - id: setup
        type: io.kestra.plugin.scripts.shell.Commands
        commands:
          - mkdir -p artifacts
          - mkdir -p project
          - echo "Setup complete - $(date)"

      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # STEP 2: Clone Repository
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - id: clone_repository
        type: io.kestra.plugin.scripts.shell.Commands
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
          image: alpine/git:latest
        commands:
          - git clone --depth 1 --branch "{{ trigger.body.branch ?? 'main' }}" "{{ trigger.body.repo_url }}" project
          - ls -la project/

      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # STEP 3: Extract Metadata & Analyze Code
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - id: analyze_project
        type: io.kestra.plugin.scripts.python.Script
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
          image: python:3.11-slim
        beforeCommands:
          - pip install pathlib2 --quiet
        script: |
          import os
          import json
          from pathlib import Path
          from datetime import datetime
          
          project_dir = Path("project")
          artifacts_dir = Path("artifacts")
          
          print(f"Analyzing project in: {project_dir.absolute()}")
          print(f"Contents: {list(project_dir.iterdir()) if project_dir.exists() else 'NOT FOUND'}")
          
          # â”€â”€â”€ Extract Metadata â”€â”€â”€
          metadata = {
              "name": None,
              "description": None,
              "language": None,
              "framework": None,
              "dependencies": [],
              "hasReadme": False,
              "hasTests": False,
              "structure": []
          }
          
          # Check package.json (Node.js)
          package_json = project_dir / "package.json"
          if package_json.exists():
              try:
                  with open(package_json) as f:
                      pkg = json.load(f)
                  metadata["name"] = pkg.get("name")
                  metadata["description"] = pkg.get("description")
                  metadata["language"] = "JavaScript/TypeScript"
                  deps = list(pkg.get("dependencies", {}).keys())
                  metadata["dependencies"] = deps[:20]
                  
                  # Detect framework
                  if "next" in deps:
                      metadata["framework"] = "Next.js"
                  elif "react" in deps:
                      metadata["framework"] = "React"
                  elif "vue" in deps:
                      metadata["framework"] = "Vue.js"
                  elif "express" in deps:
                      metadata["framework"] = "Express.js"
              except Exception as e:
                  print(f"Error reading package.json: {e}")
          
          # Check for README
          for readme_name in ["README.md", "README.rst", "README.txt", "readme.md"]:
              if (project_dir / readme_name).exists():
                  metadata["hasReadme"] = True
                  break
          
          # Check for tests
          test_patterns = ["test", "tests", "__tests__", "spec", "specs"]
          for pattern in test_patterns:
              if (project_dir / pattern).is_dir():
                  metadata["hasTests"] = True
                  break
          
          # Get file structure (top level)
          if project_dir.exists():
              for item in sorted(project_dir.iterdir()):
                  if not item.name.startswith('.'):
                      metadata["structure"].append({
                          "name": item.name,
                          "type": "directory" if item.is_dir() else "file"
                      })
          
          # Save metadata
          with open(artifacts_dir / "metadata.json", "w") as f:
              json.dump(metadata, f, indent=2)
          
          print("Metadata extracted:")
          print(json.dumps(metadata, indent=2))
          
          # â”€â”€â”€ Code Quality Analysis â”€â”€â”€
          code_quality = {
              "agentName": "Code Quality & Architecture Agent",
              "agentType": "code_quality",
              "score": 75,
              "confidence": 90,
              "strengths": [],
              "weaknesses": [],
              "judgeComment": ""
          }
          
          # Check TypeScript
          has_ts = (project_dir / "tsconfig.json").exists()
          if has_ts:
              code_quality["score"] += 10
              code_quality["strengths"].append("Using TypeScript for type safety")
          
          # Check linting
          has_eslint = (project_dir / ".eslintrc.json").exists() or (project_dir / ".eslintrc.js").exists()
          if has_eslint:
              code_quality["score"] += 5
              code_quality["strengths"].append("ESLint configured for code consistency")
          
          # Check package-lock or yarn.lock
          has_lockfile = (project_dir / "package-lock.json").exists() or (project_dir / "yarn.lock").exists()
          if has_lockfile:
              code_quality["strengths"].append("Lockfile present for reproducible builds")
          
          # Check .gitignore
          has_gitignore = (project_dir / ".gitignore").exists()
          if not has_gitignore:
              code_quality["weaknesses"].append("Missing .gitignore file")
          
          if metadata["hasTests"]:
              code_quality["score"] += 5
              code_quality["strengths"].append("Test directory present")
          else:
              code_quality["weaknesses"].append("No test directory found")
          
          code_quality["score"] = min(code_quality["score"], 100)
          code_quality["judgeComment"] = f"Code analysis score: {code_quality['score']}/100"
          
          with open(artifacts_dir / "agent_code_quality.json", "w") as f:
              json.dump(code_quality, f, indent=2)
          
          # â”€â”€â”€ UX & Design Analysis â”€â”€â”€
          ux_analysis = {
              "agentName": "UX & Design Agent",
              "agentType": "ux",
              "score": 70,
              "confidence": 85,
              "strengths": [],
              "weaknesses": [],
              "judgeComment": ""
          }
          
          # Check for styling
          has_css = any(project_dir.rglob("*.css"))
          has_tailwind = (project_dir / "tailwind.config.js").exists() or (project_dir / "tailwind.config.ts").exists()
          
          if has_tailwind:
              ux_analysis["score"] += 10
              ux_analysis["strengths"].append("Using Tailwind CSS for modern styling")
          elif has_css:
              ux_analysis["strengths"].append("Custom CSS styling present")
          
          # Check for responsive design
          if metadata.get("framework") in ["Next.js", "React", "Vue.js"]:
              ux_analysis["score"] += 10
              ux_analysis["strengths"].append(f"Using {metadata['framework']} for component-based UI")
          
          ux_analysis["score"] = min(ux_analysis["score"], 100)
          ux_analysis["judgeComment"] = f"UX analysis score: {ux_analysis['score']}/100"
          
          with open(artifacts_dir / "agent_ux.json", "w") as f:
              json.dump(ux_analysis, f, indent=2)
          
          # â”€â”€â”€ Product & Innovation Analysis â”€â”€â”€
          product_analysis = {
              "agentName": "Product & Innovation Agent",
              "agentType": "product",
              "score": 75,
              "confidence": 80,
              "strengths": [],
              "weaknesses": [],
              "judgeComment": ""
          }
          
          # Check README content length
          readme_path = project_dir / "README.md"
          if readme_path.exists():
              readme_content = readme_path.read_text()
              readme_len = len(readme_content)
              if readme_len > 2000:
                  product_analysis["score"] += 10
                  product_analysis["strengths"].append("Comprehensive README documentation")
              elif readme_len > 500:
                  product_analysis["strengths"].append("Good README documentation")
              else:
                  product_analysis["weaknesses"].append("README could be more detailed")
          
          product_analysis["score"] = min(product_analysis["score"], 100)
          product_analysis["judgeComment"] = f"Product analysis score: {product_analysis['score']}/100"
          
          with open(artifacts_dir / "agent_product.json", "w") as f:
              json.dump(product_analysis, f, indent=2)
          
          # â”€â”€â”€ Presentation & Documentation Analysis â”€â”€â”€
          presentation = {
              "agentName": "Presentation Agent",
              "agentType": "presentation",
              "score": 60,
              "confidence": 90,
              "checks": {
                  "hasReadme": metadata["hasReadme"],
                  "hasContributing": (project_dir / "CONTRIBUTING.md").exists(),
                  "hasLicense": (project_dir / "LICENSE").exists() or (project_dir / "LICENSE.md").exists(),
                  "hasChangelog": (project_dir / "CHANGELOG.md").exists()
              },
              "strengths": [],
              "weaknesses": [],
              "judgeComment": ""
          }
          
          if presentation["checks"]["hasReadme"]:
              presentation["score"] += 15
              presentation["strengths"].append("Has README file")
          if presentation["checks"]["hasContributing"]:
              presentation["score"] += 10
              presentation["strengths"].append("Has contribution guidelines")
          if presentation["checks"]["hasLicense"]:
              presentation["score"] += 10
              presentation["strengths"].append("Proper open-source license")
          if presentation["checks"]["hasChangelog"]:
              presentation["score"] += 5
              presentation["strengths"].append("Maintains a changelog")
          
          presentation["score"] = min(presentation["score"], 100)
          presentation["judgeComment"] = f"Documentation score: {presentation['score']}/100"
          
          with open(artifacts_dir / "agent_presentation.json", "w") as f:
              json.dump(presentation, f, indent=2)
          
          # â”€â”€â”€ Performance Placeholder â”€â”€â”€
          performance = {
              "agentName": "Performance Agent",
              "agentType": "performance",
              "score": 65,
              "confidence": 70,
              "strengths": ["Lighthouse testing would require browser environment"],
              "weaknesses": ["Performance metrics not fully tested in CI"],
              "judgeComment": "Performance score: 65/100 (estimated)"
          }
          
          with open(artifacts_dir / "agent_performance.json", "w") as f:
              json.dump(performance, f, indent=2)
          
          # â”€â”€â”€ Check for CodeRabbit â”€â”€â”€
          coderabbit_check = {
              "hasCodeRabbit": (project_dir / ".coderabbit.yaml").exists(),
              "canSetup": True
          }
          
          with open(artifacts_dir / "coderabbit_check.json", "w") as f:
              json.dump(coderabbit_check, f, indent=2)
          
          print("\nâœ… All agent analyses complete!")

      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # STEP 4: AI-Powered Analysis Summary (Kestra Native AI Plugin)
      # Uses OpenAI to summarize findings and make decisions
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - id: ai_summarize_analysis
        type: io.kestra.plugin.openai.ChatCompletion
        description: Use Kestra's native AI plugin to summarize project analysis and provide actionable decisions
        apiKey: "{{ secret('OPENAI_API_KEY') }}"
        model: gpt-4o-mini
        messages:
          - role: system
            content: |
              You are a hackathon project evaluator AI. Analyze the provided project data and:
              1. Summarize the project's strengths and weaknesses
              2. Decide if the project is ready for submission (YES/NO)
              3. Provide the top 3 actionable improvements
              4. Suggest which hackathon prize tracks the project qualifies for
              
              Respond in JSON format with keys: summary, submissionReady, topImprovements, eligibleTracks
          - role: user
            content: |
              Analyze this hackathon project:
              
              Project URL: {{ trigger.body.repo_url }}
              Branch: {{ trigger.body.branch ?? 'main' }}
              
              Please provide your analysis in JSON format.

      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # STEP 5: Save AI Summary to artifacts
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - id: save_ai_summary
        type: io.kestra.plugin.scripts.shell.Commands
        commands:
          - |
            cat > artifacts/ai_summary.json << 'EOF'
            {
              "model": "gpt-4o-mini",
              "provider": "Kestra Native OpenAI Plugin",
              "response": {{ outputs.ai_summarize_analysis.choices | first | jq('.message.content') }}
            }
            EOF
          - echo "âœ… AI Summary saved to artifacts/ai_summary.json"

      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # STEP 6: Generate Final Report
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

      - id: generate_report
        type: io.kestra.plugin.scripts.python.Script
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
          image: python:3.11-slim
        script: |
          import json
          from pathlib import Path
          from datetime import datetime
          
          artifacts_dir = Path("artifacts")
          
          print(f"Generating final report from: {artifacts_dir.absolute()}")
          print(f"Contents: {list(artifacts_dir.iterdir())}")
          
          # Load all agent results
          agents = []
          agent_files = [
              "agent_code_quality.json",
              "agent_ux.json",
              "agent_performance.json",
              "agent_product.json",
              "agent_presentation.json"
          ]
          
          for agent_file in agent_files:
              path = artifacts_dir / agent_file
              if path.exists():
                  with open(path) as f:
                      agents.append(json.load(f))
                  print(f"âœ“ Loaded {agent_file}")
          
          # Load metadata
          metadata = {}
          metadata_path = artifacts_dir / "metadata.json"
          if metadata_path.exists():
              with open(metadata_path) as f:
                  metadata = json.load(f)
          
          # Calculate dimension scores
          dimensions = {
              "innovation": 75,
              "technical": 80,
              "ux": 80,
              "performance": 65,
              "codeQuality": 80,
              "presentation": 70
          }
          
          for agent in agents:
              agent_type = agent.get("agentType", "")
              score = agent.get("score", 75)
              if agent_type == "code_quality":
                  dimensions["codeQuality"] = score
                  dimensions["technical"] = score
              elif agent_type == "ux":
                  dimensions["ux"] = score
              elif agent_type == "performance":
                  dimensions["performance"] = score
              elif agent_type == "product":
                  dimensions["innovation"] = score
              elif agent_type == "presentation":
                  dimensions["presentation"] = score
          
          # Calculate overall readiness score (weighted average)
          readiness_score = int(
              dimensions["innovation"] * 0.15 +
              dimensions["technical"] * 0.20 +
              dimensions["ux"] * 0.15 +
              dimensions["performance"] * 0.15 +
              dimensions["codeQuality"] * 0.20 +
              dimensions["presentation"] * 0.15
          )
          
          # Determine status
          if readiness_score >= 85:
              status = "STRONG"
          elif readiness_score >= 70:
              status = "GOOD"
          elif readiness_score >= 50:
              status = "NEEDS_WORK"
          else:
              status = "WEAK"
          
          # Collect strengths and weaknesses
          all_strengths = []
          all_weaknesses = []
          for agent in agents:
              all_strengths.extend(agent.get("strengths", []))
              all_weaknesses.extend(agent.get("weaknesses", []))
          
          # Load CodeRabbit check
          coderabbit = {"hasCodeRabbit": False, "canSetup": True}
          cr_path = artifacts_dir / "coderabbit_check.json"
          if cr_path.exists():
              with open(cr_path) as f:
                  coderabbit = json.load(f)
          
          # Load AI Summary from Kestra Native AI Plugin
          ai_summary = {"available": False}
          ai_path = artifacts_dir / "ai_summary.json"
          if ai_path.exists():
              try:
                  with open(ai_path) as f:
                      ai_summary = json.load(f)
                      ai_summary["available"] = True
                  print("âœ“ Loaded AI summary from Kestra OpenAI plugin")
              except:
                  print("âš  AI summary exists but could not be parsed")
          
          # Build award eligibility
          awards = []
          if len([d for d in metadata.get("dependencies", []) if d in ["kestra", "next", "react", "together"]]) >= 2:
              awards.append({
                  "name": "Infinity Build Award",
                  "eligible": True,
                  "reason": "Uses multiple sponsor tools"
              })
          
          if not coderabbit["hasCodeRabbit"]:
              awards.append({
                  "name": "Captain Code (CodeRabbit)",
                  "eligible": False,
                  "reason": "CodeRabbit not configured - can be set up automatically",
                  "canAutoSetup": True
              })
          
          # Build final report
          report = {
              "projectId": "{{ trigger.body.job_id }}",
              "repoUrl": "{{ trigger.body.repo_url }}",
              "branch": "{{ trigger.body.branch ?? 'main' }}",
              "readinessScore": readiness_score,
              "status": status,
              "summary": f"Project evaluation complete. Overall readiness: {readiness_score}/100 ({status}). {'Excellent project structure!' if readiness_score >= 80 else 'Good foundation with room for improvement.'}",
              "dimensions": dimensions,
              "strengths": all_strengths[:5],
              "weaknesses": all_weaknesses[:5],
              "agentFeedback": agents,
              "metadata": metadata,
              "awardEligibility": awards,
              "coderabbitStatus": coderabbit,
              "aiSummary": ai_summary,
              "generatedContent": {
                  "devpostDraft": "",
                  "pitchScript": "",
                  "architectureDiagram": ""
              },
              "completedAt": datetime.now().isoformat()
          }

          
          # Save final report
          with open(artifacts_dir / "final_report.json", "w") as f:
              json.dump(report, f, indent=2)
          
          print("\n" + "="*60)
          print("ðŸ“Š FINAL REPORT")
          print("="*60)
          print(f"Readiness Score: {readiness_score}/100")
          print(f"Status: {status}")
          print(f"Agents analyzed: {len(agents)}")
          print(f"Strengths: {len(all_strengths)}")
          print(f"Weaknesses: {len(all_weaknesses)}")
          print("="*60)
          print("\nâœ… Report saved to artifacts/final_report.json")

triggers:
  - id: api_trigger
    type: io.kestra.plugin.core.trigger.Webhook
    key: hackjudge-evaluate

errors:
  - id: handle_failure
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - echo "Evaluation failed for job {{ trigger.body.job_id }}"
      - echo "Error occurred at $(date)"
